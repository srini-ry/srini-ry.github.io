<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLM Resource List | Srinivas Reddy Aellala </title> <meta name="author" content="Srinivas Reddy Aellala"> <meta name="description" content="Running List of useful resources/links in the current LLM landscape"> <meta name="keywords" content="AI, Robotics, Startup, Edge, Hardware, Mobility, Autonomous Vehicles, ADAS, personal-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srini-ry.github.io/projects/1_project/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Srinivas Reddy</span> Aellala </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLM Resource List</h1> <p class="post-description">Running List of useful resources/links in the current LLM landscape</p> </header> <article> <h2 id="llm-resources---running-list">LLM Resources - Running List</h2> <p>This is a working shortlist of resources I find useful for projects covering small models, cost and serving details etc.</p> <h3 id="models-for-coding">Models for Coding</h3> <ul> <li> <a href="https://ollama.ai/library/codebooga" rel="external nofollow noopener" target="_blank">Ollama AI’s Codebooga</a> <ul> <li>A high-performing code instruct model created by merging two existing code models (Phind-CodeLlama-34B, WizardCoder-Python-34B)</li> </ul> </li> <li> <a href="https://huggingface.co/blog/starcoder" rel="external nofollow noopener" target="_blank">Starcoder</a> <ul> <li>Trained on Github Licensed data, from 80+ programminh languages, commits, issues and Jupyter notebooks</li> </ul> </li> </ul> <h3 id="local-small-models">Local, small models</h3> <p>Relevant links:</p> <ul> <li> <a href="https://www.xda-developers.com/mixtral-8x7b/" rel="external nofollow noopener" target="_blank">Mistral 7b</a> <ul> <li><a href="https://www.pinecone.io/learn/mixtral-8x7b/" rel="external nofollow noopener" target="_blank">Getting started blog</a></li> <li>Seems to be the best local and open source LLM model out there, so far.</li> <li>Useful for creating hybrid setups for AI Agents - where a small, local LLM is used for initial classification or PII removal and GPT4 in the backend for rest of the problem/task</li> </ul> </li> <li>Tiny llama 1B</li> <li><a href="https://ai.google.dev/tutorials/android_aicore?_gl=1*1l85n5p*_up*MQ..&amp;gclid=Cj0KCQiAz8GuBhCxARIsAOpzk8w5fgQzY0Sa74LIEnxN9PioMEULaVZ9MU3A7-Tjal6GFxpz0UyM-j4aAhV6EALw_wcB#benefits-on-device" rel="external nofollow noopener" target="_blank">Google Gemini Nano - AI Edge SDK</a></li> <li>Ollama</li> <li><a href="https://blogs.nvidia.com/blog/chat-with-rtx-available-now/" rel="external nofollow noopener" target="_blank">Nvidia’s Local Chatbot</a></li> <li> <a href="https://x.com/Karmedge/status/1727328990090998191?s=20" rel="external nofollow noopener" target="_blank">bakllava</a> on local computer in real time to describe images streaming on webcam</li> </ul> <h3 id="serving-and-deployment">Serving and Deployment</h3> <ul> <li> <a href="https://lambdalabs.com/lambda-stack-deep-learning-software" rel="external nofollow noopener" target="_blank">Lambdalabs</a> - Unified docker file and image for all DL softwares - cuda, pytorch etc. <ul> <li>Works for servers, desktops and laptops.</li> <li>Single line install commands</li> <li>Check for Jetson .</li> </ul> </li> <li><a href="https://github.com/guidance-ai/guidance" rel="external nofollow noopener" target="_blank">Guidance for Output Generation</a></li> <li>Serving large number of users with small models <ul> <li>User posts: <ul> <li>Setup is extremely complex having to deal with builds, CUDA, firmware, Triton and model compatibility issues etc.</li> <li>TensorRT-LLM has superior performance Example: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md</li> <li>Combine it with the tensorrt_llm backend in [Triton Inference Server],(https://github.com/triton-inference-server/serve)</li> <li>Used by most big commercial providers on their backends - like Cloudflare, AWS, Perplexity, Databricks, Phind, etc.</li> <li>7b on low-end Nvidia TensorRT-capable hardware with this approach can handle “only” a couple of thousand users easily.</li> </ul> </li> <li>Source: <a href="https://www.reddit.com/r/LocalLLaMA/s/TwirASRY7M" rel="external nofollow noopener" target="_blank">Reddit: Serving Small Models</a> </li> </ul> </li> <li>gpt-auth.com</li> </ul> <h3 id="inside-the-models">Inside the models</h3> <ul> <li>System instruction behind desktop version of Chatgpt: https://x.com/krishnanrohit/status/1755123169353236848?s=20</li> <li>https://github.com/karpathy/minbpe - encode token method</li> <li>Mental model on current limitations of GPT. <a href="https://www.linkedin.com/posts/drjimfan_i-see-some-vocal-objections-sora-is-not-ugcPost-7164315295774392320-VYoI?utm_source=share&amp;utm_medium=member_android" rel="external nofollow noopener" target="_blank">Is it just probability of next token or is it learning the python/the world?</a> </li> <li>https://chsasank.com/llm-system-design.html</li> <li>Long Context <ul> <li>How LLMs get lost in the middle (when the context is long). They prefer outer edges - https://arxiv.org/pdf/2307.03172.pdf</li> <li>LLMs with long context <a href="https://arxiv.org/abs/2401.01325" rel="external nofollow noopener" target="_blank">LongLM</a> </li> </ul> </li> </ul> <h3 id="rag">RAG</h3> <ul> <li> <a href="https://www.reddit.com/r/LocalLLaMA/comments/19crm8i/comment/kj1yy0y/?share_id=gGNCwzqeX7KJpGMhZo6Zw&amp;utm_content=2&amp;utm_medium=android_app&amp;utm_name=androidcss&amp;utm_source=share&amp;utm_term=1" rel="external nofollow noopener" target="_blank">Current Status of RAG</a>, as of Jan 2024</li> </ul> <h3 id="testing-and-benchmarking">Testing and Benchmarking</h3> <ul> <li>To test retreival capabilities for long context - https://github.com/gkamradt/LLMTest_NeedleInAHaystack</li> </ul> <h3 id="fine-tuning">Fine-Tuning</h3> <ul> <li> <a href="https://wandb.ai/parambharat/whisper_finetuning/reports/Fine-Tuning-Whisper-for-Low-Resource-Dravidian-Languages--VmlldzozMTYyNTg0" rel="external nofollow noopener" target="_blank">Fine-Tuning Whisper for Low-Resource Languages</a> - Great Article on fine tuning process</li> <li>Finetuning methods <ul> <li>LoRA https://github.com/microsoft/LoRA <ul> <li>LoRa is a parameter-efficient fine-tuning style for LLMs and diffusion models. It is ideal to run after an integer 8 quantization. LoRa can be combined with other methods within PEFT, such as prefix tuning.</li> </ul> </li> <li>PEFT https://github.com/huggingface/peft</li> </ul> </li> </ul> <h3 id="hardware">Hardware</h3> <ul> <li>Explore <a href="https://www.reddit.com/r/LocalLLaMA/s/WOu0zCjQ1x" rel="external nofollow noopener" target="_blank">AI-Specific ASIC Options</a> </li> <li>Cerebras.ai</li> <li>etched.ai</li> <li>Specs comparison <ul> <li>4070 GPU = 235 TOPS @200W.</li> <li>Jetson AGX = 275 TOPS @ 15-50W</li> </ul> </li> </ul> <h3 id="inference-optimization">Inference Optimization</h3> <ul> <li>Learn about <a href="https://deci.ai/blog/optimizing-openais-whisper-for-production-with-infery-ffm/" rel="external nofollow noopener" target="_blank">Inference Optimization Techniques</a> </li> <li> <a href="https://www.reddit.com/r/LocalLLaMA/s/uIrk5MYGyJ" rel="external nofollow noopener" target="_blank">LORA extract</a> from any large model</li> </ul> <h3 id="cost-management-in-ai-development">Cost Management in AI Development</h3> <p>Practical tips for managing costs while using Assistant or GPT model APIs (From multiple sources)</p> <ul> <li>Don’t have long chats, to cut down on token usage. Even though it’s hosted, it still has to process the entire chat every time, and you pay for all those tokens. Start a chat over at the beginning of each new task.</li> <li>If you must have long chats, periodically generate a summary and start the chat over with the summary as the first message followed by the last few messages.</li> <li>Use code interpreter instead of knowledge retrieval for structured data, such as a CSV file. Only use knowledge retrieval when dealing with unstructured natural language (i.e. text files, pdfs, etc).</li> <li>When using code interpreter, you can sometimes save tokens if you hand write the code it needs. For complicated data, you can have it use sqlite3 as it allows you to write very complex queries with very few tokens.</li> <li>Train GPT-3.5 (or GPT-4 Turbo) using GPT-4. Create some examples using GPT-4 and then paste that chat into a GPT-3.5 (or GPT-4 Turbo) chat. Research “shot prompting”. When done well, this results in a GPT-3.75</li> <li>Experiment with various prompt/chat techniques while actively watching your <a href="https://platform.openai.com/account/usage" rel="external nofollow noopener" target="_blank">usage</a>. (Keep in mind it can be delayed a few minutes.)</li> <li>Limit to number of tools(functions) and how to scale https://community.openai.com/t/limit-on-the-number-of-functions-definitions-for-assistant/537992/2</li> </ul> <p>Costs of LLM inferencing</p> <ul> <li>https://cursor.sh/blog/llama-inference - Llama2 is cheaper for prompt heavy tasks like classification, costlier than 3.5 for competition tokens.</li> <li>https://tokentally.streamlit.app/</li> <li>https://blog.eleuther.ai/transformer-math/</li> <li>Batch 1 mode - llama inference makes sense - can do on smaller edge units also https://finbarr.ca/how-is-llama-cpp-possible/</li> <li>https://twitter.com/karpathy/status/1691571869051445433?t=3bp70Zz0DaBVO64KKlnyaw&amp;s=19</li> <li>https://github.com/chsasank/device-benchmarks/blob/main/llama_bench.sh - bench’s</li> </ul> <h3 id="use-cases-for-both-smaller-and-general-llms">Use cases for both smaller and general LLMs</h3> <ul> <li>Interactiveness in every face of product - generate sample followup questions against a certain component of your UI - be it a kpi metric, data dashboard, blog etc. base it on the user cache for more personalization.. . Perplexity, LinkedIn is already dng it. You can imagine this in every product UI where personalizatiok drives more interaction and there by positive user engagement</li> </ul> <h3 id="agent-building">Agent building</h3> <ul> <li>Openai assistant : Best so far</li> <li>Langchain agent - ReAct, Mrkl etc. frameworks. Observed mediocre performance</li> <li>AutoGPT</li> <li>AgentGPT</li> <li>Geminipro + vertexai - tool calling is still upcoming https://cloud.google.com/dialogflow/vertex/docs/quick/create-application</li> <li>Does Claude have one?</li> </ul> <p>### Meeting Tools</p> <ul> <li>https://www.usebubbles.com/</li> </ul> <h3 id="automotive-related-llm-applications">Automotive related LLM applications</h3> <ul> <li>Cerence - callm. Rag on vehicle specific data, voice also. Sold to NA OEM and EU OEM.</li> <li>Baidu Ernie bot</li> <li>Inrix compass - on traffic data</li> <li>IBM eam</li> </ul> <h3 id="blogs">Blogs</h3> <ul> <li>https://www.jasonwei.net/thoughts - AI researcher at Openai</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Srinivas Reddy Aellala. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>